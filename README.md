# ğŸ™ï¸ Customer Support Voice Assistant  

A conversational **voice assistant** powered by the **Groq API** for ultra-fast language model inference, **ElevenLabs** for high-quality Text-to-Speech (TTS), and **SpeechRecognition** for accurate Voice-to-Text (VTT).  

The project features a **Python Flask backend** and a **modern single-file HTML/JavaScript frontend** (with optional React/JSX version).  

---

## ğŸš€ Features  

- **Real-Time Conversation** â†’ Leverages Groqâ€™s low-latency performance for fluid, natural conversation.  
- **Multimodal Interaction** â†’ Handles user **voice input (VTT)** and provides **synthesized voice replies (TTS)**.  
- **Session Management** â†’ Maintains conversation history & context with unique session IDs.  
- **Extensible Backend** â†’ Built with **LangGraph/ReAct structure** for easy addition of tools and functionality.  

---

## âš™ï¸ Prerequisites  

To run this project, youâ€™ll need:  

- Python **3.8+**  
- **API Keys**:  
  - Groq API Key â†’ For accessing LLMs (e.g., Llama 3) and Whisper transcription.  
  - ElevenLabs API Key â†’ For generating TTS audio.  
- **FFmpeg** â†’ Required by `pydub` for audio conversion. Must be installed and added to your systemâ€™s `PATH`.  

---

## ğŸ› ï¸ Setup & Installation  

### 1. Configure Environment Variables  

Create a `.env` file in the root directory:  

```ini
# .env
GROQ_API_KEY="YOUR_GROQ_API_KEY"
ELEVENLABS_API_KEY="YOUR_ELEVENLABS_API_KEY"
# Create a virtual environment
python -m venv venv

# Activate (Linux/macOS)
source venv/bin/activate

# Activate (Windows)
.\venv\Scripts\activate

# Install dependencies
pip install Flask groq elevenlabs SpeechRecognition pydub python-dotenv langchain_core langgraph
ğŸ—ï¸ Architecture Overview

The system follows a client-server architecture:

Client (voice_assistant_client.html / React Component)

Handles UI, microphone access, and audio recording.

Sends audio blobs (WAV) + session ID â†’ Flask backend.

Receives synthesized audio responses and plays them back.

Server (personal_assistant_server.py)

VTT â†’ Uses Groq Whisper to transcribe audio.

Agent â†’ Powered by a LangGraph ReAct agent to decide actions (tools like add_task, web_search, or direct responses).

TTS â†’ Uses ElevenLabs for natural-sounding audio replies.

Returns:

Userâ€™s transcription

AI-generated audio response

Updated session IDExample Workflow

User clicks ğŸ¤ microphone â†’ speaks a query.

Client records & sends audio â†’ backend /api/chat.

Backend â†’ transcribes (Whisper) â†’ processes (LangGraph agent) â†’ generates reply.

Backend â†’ sends back AI response text + TTS audio.

Client â†’ plays AIâ€™s voice reply â†’ conversation continues.
